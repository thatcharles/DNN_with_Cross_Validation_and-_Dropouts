{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as sk\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "###### Do not modify here ###### \n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# training on MNIST but only on digits 0 to 4\n",
    "X_train1 = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5]\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5]\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5]\n",
    "\n",
    "###### Do not modify here ###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform to one_hot array\n",
    "def one_hot(input_data):\n",
    "    one_hot = []\n",
    "    for item in input_data:\n",
    "        if item == 0.:\n",
    "            one_h = [1.,0.,0.,0.,0.]\n",
    "        elif item == 1.:\n",
    "            one_h = [0.,1.,0.,0.,0.]\n",
    "        elif item == 2.:\n",
    "            one_h = [0.,0.,1.,0.,0.]\n",
    "        elif item == 3.:\n",
    "            one_h = [0.,0.,0.,1.,0.]\n",
    "        elif item == 4.:\n",
    "            one_h = [0.,0.,0.,0.,1.]\n",
    "\n",
    "        one_hot.append(one_h)\n",
    "    one_hot = np.array(one_hot)\n",
    "    #one_hot = one_hot.reshape(len(one_hot),10,1)\n",
    "    #one_hot = one_hot.reshape(len(one_hot), 7,1)\n",
    "    #return tf.constant([one_hot])\n",
    "    return one_hot\n",
    "\n",
    "def modelInput(input, INPUT_NODE, NEURONS, reuse, keep_prob):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = get_scope_variable(\"DNN\", \"weights\", shape=[INPUT_NODE, NEURONS], initializer= tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG', uniform=True), reuse = reuse)\n",
    "    # Create variable named \"biases\".\n",
    "    biases = get_scope_variable(\"DNN\", \"biases\", shape=[NEURONS], initializer= tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG', uniform=True), reuse = reuse)\n",
    "    hidden1 = tf.nn.elu(tf.matmul(input, weights) + biases)\n",
    "    dropout1 = tf.nn.dropout(hidden1,keep_prob)\n",
    "    return dropout1\n",
    "\n",
    "def modelhHidden(input, INPUT_NODE, NEURONS, reuse, keep_prob):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = get_scope_variable(\"DNN\", \"weights\", shape=[NEURONS, NEURONS], initializer= tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG', uniform=True), reuse = reuse)\n",
    "    # Create variable named \"biases\".\n",
    "    biases = get_scope_variable(\"DNN\", \"biases\", shape=[NEURONS], initializer= tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG', uniform=True), reuse = reuse)\n",
    "    hidden = tf.nn.elu(tf.matmul(input, weights) + biases)\n",
    "    dropout = tf.nn.dropout(hidden,keep_prob)\n",
    "    return dropout\n",
    "\n",
    "def modelOutput(input, INPUT_NODE, NEURONS, reuse):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = get_scope_variable(\"DNN\", \"weights\", shape=[NEURONS, 5], initializer= tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG', uniform=True), reuse = reuse)\n",
    "    # Create variable named \"biases\".\n",
    "    biases = get_scope_variable(\"DNN\", \"biases\", shape=[5], initializer= tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG', uniform=True), reuse = reuse)\n",
    "    output = tf.matmul(input, weights) + biases\n",
    "    output_after_softmax = tf.nn.softmax(output)\n",
    "    return output, output_after_softmax\n",
    "\n",
    "def training(batch_xs,INPUT_NODE,NEURONS, reuse, keep_prob):\n",
    "    sess = tf.InteractiveSession()\n",
    "    with tf.variable_scope(\"layer1\",reuse = reuse):\n",
    "        # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "        output1 = modelInput(batch_xs, INPUT_NODE, NEURONS, reuse, keep_prob)\n",
    "    with tf.variable_scope(\"layer2\",reuse = reuse):\n",
    "        output2 = modelhHidden(output1, INPUT_NODE, NEURONS, reuse, keep_prob)\n",
    "    with tf.variable_scope(\"layer3\",reuse = reuse):\n",
    "        output3 = modelhHidden(output2, INPUT_NODE, NEURONS, reuse, keep_prob)\n",
    "    with tf.variable_scope(\"layer4\",reuse = reuse):\n",
    "        output4 = modelhHidden(output3, INPUT_NODE, NEURONS, reuse, keep_prob)\n",
    "    with tf.variable_scope(\"layer5\",reuse = reuse):\n",
    "        output5 = modelhHidden(output4, INPUT_NODE, NEURONS, reuse, keep_prob)\n",
    "    with tf.variable_scope(\"output\",reuse = reuse):\n",
    "        return modelOutput(output5, INPUT_NODE, NEURONS, reuse)\n",
    "    \n",
    "def get_scope_variable(scope_name, var, shape, initializer, reuse):\n",
    "    with tf.variable_scope(scope_name,reuse = reuse) as scope:\n",
    "        try:\n",
    "            v = tf.get_variable(var, shape=shape, initializer=initializer)\n",
    "        except ValueError:\n",
    "            scope.reuse_variables()\n",
    "            v = tf.get_variable(var, shape=shape, initializer=initializer)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and cross validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Dropout keep rate: 0.500000\n",
      "Training with 10-flod cross validation\n",
      "--------------------\n",
      "Flod number: 1\n",
      "Stop at epoch 6, Final accuracy: 0.972876\n",
      "Label 0 Precision: 1.000000, Recall: 0.975845\n",
      "Label 1 Precision: 0.989474, Recall: 0.973373\n",
      "Label 2 Precision: 0.904610, Recall: 0.979346\n",
      "Label 3 Precision: 0.989691, Recall: 0.953642\n",
      "Label 4 Precision: 0.972318, Recall: 0.972318\n",
      "Flod number: 2\n",
      "Stop at epoch 8, Final accuracy: 0.979085\n",
      "Label 0 Precision: 0.965404, Recall: 0.998296\n",
      "Label 1 Precision: 0.995595, Recall: 0.975540\n",
      "Label 2 Precision: 0.994253, Recall: 0.902609\n",
      "Label 3 Precision: 0.921450, Recall: 0.995106\n",
      "Label 4 Precision: 0.984694, Recall: 0.981356\n",
      "Flod number: 3\n",
      "Stop at epoch 7, Final accuracy: 0.982026\n",
      "Label 0 Precision: 0.998192, Recall: 0.980462\n",
      "Label 1 Precision: 0.998514, Recall: 0.958631\n",
      "Label 2 Precision: 0.906203, Recall: 0.993367\n",
      "Label 3 Precision: 0.991304, Recall: 0.956376\n",
      "Label 4 Precision: 0.989967, Recall: 0.991625\n",
      "Flod number: 4\n",
      "Stop at epoch 8, Final accuracy: 0.980065\n",
      "Label 0 Precision: 0.960848, Recall: 1.000000\n",
      "Label 1 Precision: 0.992701, Recall: 0.992701\n",
      "Label 2 Precision: 0.968966, Recall: 0.957411\n",
      "Label 3 Precision: 0.981324, Recall: 0.964942\n",
      "Label 4 Precision: 0.994941, Recall: 0.983333\n",
      "Flod number: 5\n",
      "Stop at epoch 5, Final accuracy: 0.977124\n",
      "Label 0 Precision: 0.993277, Recall: 0.988294\n",
      "Label 1 Precision: 0.995363, Recall: 0.977238\n",
      "Label 2 Precision: 0.967577, Recall: 0.959391\n",
      "Label 3 Precision: 0.974958, Recall: 0.962109\n",
      "Label 4 Precision: 0.952607, Recall: 0.996694\n",
      "Flod number: 6\n",
      "Stop at epoch 6, Final accuracy: 0.979085\n",
      "Label 0 Precision: 0.991103, Recall: 0.980634\n",
      "Label 1 Precision: 0.993701, Recall: 0.976780\n",
      "Label 2 Precision: 0.936747, Recall: 0.976452\n",
      "Label 3 Precision: 0.985270, Recall: 0.964744\n",
      "Label 4 Precision: 0.981293, Recall: 0.986325\n",
      "Flod number: 7\n",
      "Stop at epoch 10, Final accuracy: 0.978097\n",
      "Label 0 Precision: 0.984848, Recall: 0.979899\n",
      "Label 1 Precision: 0.991018, Recall: 0.976401\n",
      "Label 2 Precision: 0.951613, Recall: 0.962480\n",
      "Label 3 Precision: 0.989916, Recall: 0.956169\n",
      "Label 4 Precision: 0.946735, Recall: 0.992793\n",
      "Flod number: 8\n",
      "Stop at epoch 7, Final accuracy: 0.983001\n",
      "Label 0 Precision: 0.993220, Recall: 0.978297\n",
      "Label 1 Precision: 0.995542, Recall: 0.982405\n",
      "Label 2 Precision: 0.947368, Recall: 0.986301\n",
      "Label 3 Precision: 0.982343, Recall: 0.976077\n",
      "Label 4 Precision: 0.982301, Recall: 0.978836\n",
      "Flod number: 9\n",
      "Stop at epoch 6, Final accuracy: 0.977444\n",
      "Label 0 Precision: 0.978758, Recall: 0.977162\n",
      "Label 1 Precision: 0.987578, Recall: 0.981481\n",
      "Label 2 Precision: 0.954472, Recall: 0.965461\n",
      "Label 3 Precision: 0.971138, Recall: 0.959732\n",
      "Label 4 Precision: 0.981636, Recall: 0.989899\n",
      "Flod number: 10\n",
      "Stop at epoch 10, Final accuracy: 0.980059\n",
      "Label 0 Precision: 0.991228, Recall: 0.960884\n",
      "Label 1 Precision: 0.992582, Recall: 0.995536\n",
      "Label 2 Precision: 0.902087, Recall: 0.970639\n",
      "Label 3 Precision: 0.985507, Recall: 0.942989\n",
      "Label 4 Precision: 0.984238, Recall: 0.984238\n",
      "Average accuracy of 10 flod cross validation: 0.978886\n",
      "--------------------\n",
      "Dropout keep rate: 0.600000\n",
      "Training with 10-flod cross validation\n",
      "--------------------\n",
      "Flod number: 1\n",
      "Stop at epoch 9, Final accuracy: 0.980392\n",
      "Label 0 Precision: 0.991909, Recall: 0.991909\n",
      "Label 1 Precision: 0.995413, Recall: 0.976012\n",
      "Label 2 Precision: 0.947020, Recall: 0.969492\n",
      "Label 3 Precision: 0.968354, Recall: 0.985507\n",
      "Label 4 Precision: 0.994565, Recall: 0.973404\n",
      "Flod number: 2\n",
      "Stop at epoch 14, Final accuracy: 0.986928\n",
      "Label 0 Precision: 0.993043, Recall: 0.987889\n",
      "Label 1 Precision: 0.998501, Recall: 0.980854\n",
      "Label 2 Precision: 0.977162, Recall: 0.977162\n",
      "Label 3 Precision: 0.968750, Recall: 0.983306\n",
      "Label 4 Precision: 0.981575, Recall: 0.991540\n",
      "Flod number: 3\n",
      "Stop at epoch 11, Final accuracy: 0.983987\n",
      "Label 0 Precision: 0.993186, Recall: 0.983137\n",
      "Label 1 Precision: 0.992614, Recall: 0.989691\n",
      "Label 2 Precision: 0.958678, Recall: 0.983051\n",
      "Label 3 Precision: 0.996616, Recall: 0.970346\n",
      "Label 4 Precision: 0.968333, Recall: 0.983080\n",
      "Flod number: 4\n",
      "Stop at epoch 6, Final accuracy: 0.981699\n",
      "Label 0 Precision: 0.987868, Recall: 0.989583\n",
      "Label 1 Precision: 0.995434, Recall: 0.981982\n",
      "Label 2 Precision: 0.976860, Recall: 0.965686\n",
      "Label 3 Precision: 0.960061, Recall: 0.987362\n",
      "Label 4 Precision: 0.989474, Recall: 0.984293\n",
      "Flod number: 5\n",
      "Stop at epoch 6, Final accuracy: 0.983660\n",
      "Label 0 Precision: 0.984615, Recall: 0.996540\n",
      "Label 1 Precision: 0.995475, Recall: 0.986547\n",
      "Label 2 Precision: 0.960345, Recall: 0.972077\n",
      "Label 3 Precision: 0.991817, Recall: 0.969600\n",
      "Label 4 Precision: 0.982287, Recall: 0.991870\n",
      "Flod number: 6\n",
      "Stop at epoch 14, Final accuracy: 0.983987\n",
      "Label 0 Precision: 0.987250, Recall: 0.987250\n",
      "Label 1 Precision: 0.986900, Recall: 0.984035\n",
      "Label 2 Precision: 0.978998, Recall: 0.964968\n",
      "Label 3 Precision: 0.970588, Recall: 0.983444\n",
      "Label 4 Precision: 0.983137, Recall: 0.988136\n",
      "Flod number: 7\n",
      "Stop at epoch 12, Final accuracy: 0.988558\n",
      "Label 0 Precision: 0.977162, Recall: 0.995017\n",
      "Label 1 Precision: 0.996956, Recall: 0.982009\n",
      "Label 2 Precision: 0.957118, Recall: 0.977233\n",
      "Label 3 Precision: 0.995041, Recall: 0.964744\n",
      "Label 4 Precision: 0.978369, Recall: 0.988235\n",
      "Flod number: 8\n",
      "Stop at epoch 7, Final accuracy: 0.983655\n",
      "Label 0 Precision: 0.980551, Recall: 0.995066\n",
      "Label 1 Precision: 0.998445, Recall: 0.984663\n",
      "Label 2 Precision: 0.946037, Recall: 0.979058\n",
      "Label 3 Precision: 0.996737, Recall: 0.962205\n",
      "Label 4 Precision: 0.986509, Recall: 0.989848\n",
      "Flod number: 9\n",
      "Stop at epoch 10, Final accuracy: 0.985289\n",
      "Label 0 Precision: 0.998397, Recall: 0.990461\n",
      "Label 1 Precision: 0.994143, Recall: 0.985486\n",
      "Label 2 Precision: 0.951641, Recall: 0.985689\n",
      "Label 3 Precision: 0.976667, Recall: 0.978297\n",
      "Label 4 Precision: 0.996510, Recall: 0.979417\n",
      "Flod number: 10\n",
      "Stop at epoch 11, Final accuracy: 0.986924\n",
      "Label 0 Precision: 1.000000, Recall: 0.986486\n",
      "Label 1 Precision: 1.000000, Recall: 0.972263\n",
      "Label 2 Precision: 0.938686, Recall: 0.990755\n",
      "Label 3 Precision: 0.989529, Recall: 0.970890\n",
      "Label 4 Precision: 0.987296, Recall: 0.990893\n",
      "Average accuracy of 10 flod cross validation: 0.981697\n",
      "--------------------\n",
      "Dropout keep rate: 0.700000\n",
      "Training with 10-flod cross validation\n",
      "--------------------\n",
      "Flod number: 1\n",
      "Stop at epoch 11, Final accuracy: 0.985948\n",
      "Label 0 Precision: 0.991611, Recall: 0.989950\n",
      "Label 1 Precision: 0.992847, Recall: 0.990014\n",
      "Label 2 Precision: 0.971284, Recall: 0.974576\n",
      "Label 3 Precision: 0.993559, Recall: 0.984051\n",
      "Label 4 Precision: 0.976449, Recall: 0.988991\n",
      "Flod number: 2\n",
      "Stop at epoch 12, Final accuracy: 0.987908\n",
      "Label 0 Precision: 0.986689, Recall: 0.993300\n",
      "Label 1 Precision: 0.996928, Recall: 0.983333\n",
      "Label 2 Precision: 0.969551, Recall: 0.982143\n",
      "Label 3 Precision: 0.978369, Recall: 0.983278\n",
      "Label 4 Precision: 0.987993, Recall: 0.977929\n",
      "Flod number: 3\n",
      "Stop at epoch 14, Final accuracy: 0.986928\n",
      "Label 0 Precision: 0.988115, Recall: 0.988115\n",
      "Label 1 Precision: 0.992647, Recall: 0.989736\n",
      "Label 2 Precision: 0.942275, Recall: 0.985790\n",
      "Label 3 Precision: 0.989655, Recall: 0.964706\n",
      "Label 4 Precision: 0.995177, Recall: 0.980983\n",
      "Flod number: 4\n",
      "Stop at epoch 13, Final accuracy: 0.987908\n",
      "Label 0 Precision: 0.998347, Recall: 0.985318\n",
      "Label 1 Precision: 0.998423, Recall: 0.990610\n",
      "Label 2 Precision: 0.968954, Recall: 0.981788\n",
      "Label 3 Precision: 0.986025, Recall: 0.984496\n",
      "Label 4 Precision: 0.978761, Recall: 0.989267\n",
      "Flod number: 5\n",
      "Stop at epoch 9, Final accuracy: 0.987582\n",
      "Label 0 Precision: 0.985294, Recall: 0.998344\n",
      "Label 1 Precision: 0.995441, Recall: 0.979073\n",
      "Label 2 Precision: 0.966071, Recall: 0.980072\n",
      "Label 3 Precision: 0.982759, Recall: 0.988959\n",
      "Label 4 Precision: 0.989865, Recall: 0.975042\n",
      "Flod number: 6\n",
      "Stop at epoch 13, Final accuracy: 0.986274\n",
      "Label 0 Precision: 0.990792, Recall: 0.988971\n",
      "Label 1 Precision: 0.986301, Recall: 0.992343\n",
      "Label 2 Precision: 0.974563, Recall: 0.988710\n",
      "Label 3 Precision: 0.993569, Recall: 0.977848\n",
      "Label 4 Precision: 0.986864, Recall: 0.983633\n",
      "Flod number: 7\n",
      "Stop at epoch 12, Final accuracy: 0.988558\n",
      "Label 0 Precision: 0.991790, Recall: 0.993421\n",
      "Label 1 Precision: 0.993856, Recall: 0.987786\n",
      "Label 2 Precision: 0.976562, Recall: 0.988924\n",
      "Label 3 Precision: 0.994633, Recall: 0.973730\n",
      "Label 4 Precision: 0.985000, Recall: 0.996627\n",
      "Flod number: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop at epoch 14, Final accuracy: 0.989212\n",
      "Label 0 Precision: 0.985437, Recall: 0.995098\n",
      "Label 1 Precision: 0.992847, Recall: 0.995696\n",
      "Label 2 Precision: 0.978333, Recall: 0.984899\n",
      "Label 3 Precision: 0.995058, Recall: 0.974194\n",
      "Label 4 Precision: 0.994393, Recall: 0.996255\n",
      "Flod number: 9\n",
      "Stop at epoch 11, Final accuracy: 0.989539\n",
      "Label 0 Precision: 0.991424, Recall: 0.996552\n",
      "Label 1 Precision: 0.994286, Recall: 0.992867\n",
      "Label 2 Precision: 0.984746, Recall: 0.974832\n",
      "Label 3 Precision: 0.984899, Recall: 0.979967\n",
      "Label 4 Precision: 0.984746, Recall: 0.996569\n",
      "Flod number: 10\n",
      "Stop at epoch 8, Final accuracy: 0.983655\n",
      "Label 0 Precision: 0.981132, Recall: 0.987910\n",
      "Label 1 Precision: 0.988321, Recall: 0.988321\n",
      "Label 2 Precision: 0.979167, Recall: 0.957555\n",
      "Label 3 Precision: 0.963141, Recall: 0.985246\n",
      "Label 4 Precision: 0.986464, Recall: 0.978188\n",
      "Average accuracy of 10 flod cross validation: 0.983582\n",
      "--------------------\n",
      "Dropout keep rate: 0.800000\n",
      "Training with 10-flod cross validation\n",
      "--------------------\n",
      "Flod number: 1\n",
      "Stop at epoch 13, Final accuracy: 0.988889\n",
      "Label 0 Precision: 0.993115, Recall: 0.994828\n",
      "Label 1 Precision: 0.992515, Recall: 0.989552\n",
      "Label 2 Precision: 0.975530, Recall: 0.983553\n",
      "Label 3 Precision: 0.990307, Recall: 0.985531\n",
      "Label 4 Precision: 0.991364, Recall: 0.989655\n",
      "Flod number: 2\n",
      "Stop at epoch 14, Final accuracy: 0.991830\n",
      "Label 0 Precision: 0.996644, Recall: 0.996644\n",
      "Label 1 Precision: 0.991202, Recall: 0.988304\n",
      "Label 2 Precision: 0.991708, Recall: 0.991708\n",
      "Label 3 Precision: 0.991830, Recall: 0.983793\n",
      "Label 4 Precision: 0.987654, Recall: 1.000000\n",
      "Flod number: 3\n",
      "Stop at epoch 14, Final accuracy: 0.988562\n",
      "Label 0 Precision: 0.995050, Recall: 0.986907\n",
      "Label 1 Precision: 0.995392, Recall: 0.987805\n",
      "Label 2 Precision: 0.983444, Recall: 0.976974\n",
      "Label 3 Precision: 0.978003, Recall: 0.986348\n",
      "Label 4 Precision: 0.980263, Recall: 0.994992\n",
      "Flod number: 4\n",
      "Stop at epoch 11, Final accuracy: 0.988889\n",
      "Label 0 Precision: 0.987993, Recall: 0.993103\n",
      "Label 1 Precision: 0.992515, Recall: 0.986607\n",
      "Label 2 Precision: 0.966667, Recall: 0.983051\n",
      "Label 3 Precision: 0.985342, Recall: 0.977383\n",
      "Label 4 Precision: 0.989916, Recall: 0.983306\n",
      "Flod number: 5\n",
      "Stop at epoch 14, Final accuracy: 0.992810\n",
      "Label 0 Precision: 0.993289, Recall: 0.996633\n",
      "Label 1 Precision: 0.997080, Recall: 0.994178\n",
      "Label 2 Precision: 0.988889, Recall: 0.979817\n",
      "Label 3 Precision: 0.995392, Recall: 0.993865\n",
      "Label 4 Precision: 0.988095, Recall: 0.998282\n",
      "Flod number: 6\n",
      "Stop at epoch 14, Final accuracy: 0.989216\n",
      "Label 0 Precision: 0.993031, Recall: 0.935961\n",
      "Label 1 Precision: 0.993921, Recall: 0.995434\n",
      "Label 2 Precision: 0.979100, Recall: 0.983845\n",
      "Label 3 Precision: 0.994845, Recall: 0.971477\n",
      "Label 4 Precision: 0.924679, Recall: 0.996546\n",
      "Flod number: 7\n",
      "Stop at epoch 14, Final accuracy: 0.990193\n",
      "Label 0 Precision: 0.986555, Recall: 0.994915\n",
      "Label 1 Precision: 0.997006, Recall: 0.994030\n",
      "Label 2 Precision: 0.981967, Recall: 0.983580\n",
      "Label 3 Precision: 0.995025, Recall: 0.986842\n",
      "Label 4 Precision: 0.987993, Recall: 0.989691\n",
      "Flod number: 8\n",
      "Stop at epoch 11, Final accuracy: 0.984962\n",
      "Label 0 Precision: 0.994774, Recall: 0.994774\n",
      "Label 1 Precision: 0.991354, Recall: 0.980057\n",
      "Label 2 Precision: 0.966312, Recall: 0.971480\n",
      "Label 3 Precision: 0.978395, Recall: 0.989080\n",
      "Label 4 Precision: 0.989637, Recall: 0.986231\n",
      "Flod number: 9\n",
      "Stop at epoch 14, Final accuracy: 0.986597\n",
      "Label 0 Precision: 0.979832, Recall: 0.996581\n",
      "Label 1 Precision: 0.998507, Recall: 0.985272\n",
      "Label 2 Precision: 0.977087, Recall: 0.981908\n",
      "Label 3 Precision: 0.985050, Recall: 0.980165\n",
      "Label 4 Precision: 0.986231, Recall: 0.984536\n",
      "Flod number: 10\n",
      "Stop at epoch 13, Final accuracy: 0.988231\n",
      "Label 0 Precision: 0.982055, Recall: 0.996689\n",
      "Label 1 Precision: 0.993976, Recall: 0.992481\n",
      "Label 2 Precision: 0.981728, Recall: 0.973641\n",
      "Label 3 Precision: 0.994810, Recall: 0.982906\n",
      "Label 4 Precision: 0.985050, Recall: 0.991639\n",
      "Average accuracy of 10 flod cross validation: 0.984941\n",
      "--------------------\n",
      "Dropout keep rate: 0.900000\n",
      "Training with 10-flod cross validation\n",
      "--------------------\n",
      "Flod number: 1\n",
      "Stop at epoch 9, Final accuracy: 0.986601\n",
      "Label 0 Precision: 0.989189, Recall: 0.990975\n",
      "Label 1 Precision: 0.981402, Recall: 1.000000\n",
      "Label 2 Precision: 0.975207, Recall: 0.984975\n",
      "Label 3 Precision: 0.994854, Recall: 0.966667\n",
      "Label 4 Precision: 0.990291, Recall: 0.985507\n",
      "Flod number: 2\n",
      "Stop at epoch 10, Final accuracy: 0.987908\n",
      "Label 0 Precision: 0.998291, Recall: 0.991511\n",
      "Label 1 Precision: 0.988439, Recall: 0.995633\n",
      "Label 2 Precision: 0.986667, Recall: 0.973684\n",
      "Label 3 Precision: 0.982114, Recall: 0.986928\n",
      "Label 4 Precision: 0.984155, Recall: 0.991135\n",
      "Flod number: 3\n",
      "Stop at epoch 9, Final accuracy: 0.986274\n",
      "Label 0 Precision: 0.991438, Recall: 0.989744\n",
      "Label 1 Precision: 0.989811, Recall: 0.991254\n",
      "Label 2 Precision: 0.991304, Recall: 0.962838\n",
      "Label 3 Precision: 0.978689, Recall: 0.990050\n",
      "Label 4 Precision: 0.980132, Recall: 0.996633\n",
      "Flod number: 4\n",
      "Stop at epoch 12, Final accuracy: 0.993464\n",
      "Label 0 Precision: 0.989097, Recall: 0.996860\n",
      "Label 1 Precision: 0.992378, Recall: 0.986364\n",
      "Label 2 Precision: 0.970228, Recall: 0.989286\n",
      "Label 3 Precision: 0.998358, Recall: 0.983819\n",
      "Label 4 Precision: 0.996564, Recall: 0.991453\n",
      "Flod number: 5\n",
      "Stop at epoch 12, Final accuracy: 0.987255\n",
      "Label 0 Precision: 0.991379, Recall: 0.984589\n",
      "Label 1 Precision: 1.000000, Recall: 0.990977\n",
      "Label 2 Precision: 0.969646, Recall: 0.979557\n",
      "Label 3 Precision: 0.981102, Recall: 0.988889\n",
      "Label 4 Precision: 0.984823, Recall: 0.983165\n",
      "Flod number: 6\n",
      "Stop at epoch 11, Final accuracy: 0.986274\n",
      "Label 0 Precision: 0.986885, Recall: 0.995041\n",
      "Label 1 Precision: 0.983800, Recall: 0.992571\n",
      "Label 2 Precision: 0.963516, Recall: 0.981419\n",
      "Label 3 Precision: 0.991722, Recall: 0.980360\n",
      "Label 4 Precision: 0.998227, Recall: 0.972366\n",
      "Flod number: 7\n",
      "Stop at epoch 11, Final accuracy: 0.988885\n",
      "Label 0 Precision: 0.991289, Recall: 0.998246\n",
      "Label 1 Precision: 0.986425, Recall: 0.992413\n",
      "Label 2 Precision: 0.981788, Recall: 0.970540\n",
      "Label 3 Precision: 0.990415, Recall: 0.979463\n",
      "Label 4 Precision: 0.981419, Recall: 0.991468\n",
      "Flod number: 8\n",
      "Stop at epoch 8, Final accuracy: 0.987251\n",
      "Label 0 Precision: 0.986348, Recall: 0.994836\n",
      "Label 1 Precision: 0.985163, Recall: 0.986627\n",
      "Label 2 Precision: 0.988055, Recall: 0.936893\n",
      "Label 3 Precision: 0.974026, Recall: 0.983607\n",
      "Label 4 Precision: 0.961474, Recall: 0.994801\n",
      "Flod number: 9\n",
      "Stop at epoch 7, Final accuracy: 0.985943\n",
      "Label 0 Precision: 0.993721, Recall: 0.985981\n",
      "Label 1 Precision: 0.995468, Recall: 0.989489\n",
      "Label 2 Precision: 0.967071, Recall: 0.985866\n",
      "Label 3 Precision: 0.998339, Recall: 0.967794\n",
      "Label 4 Precision: 0.970740, Recall: 1.000000\n",
      "Flod number: 10\n",
      "Stop at epoch 11, Final accuracy: 0.988231\n",
      "Label 0 Precision: 0.984536, Recall: 0.994792\n",
      "Label 1 Precision: 0.998536, Recall: 0.992722\n",
      "Label 2 Precision: 0.988636, Recall: 0.974400\n",
      "Label 3 Precision: 0.993197, Recall: 0.984823\n",
      "Label 4 Precision: 0.974576, Recall: 0.994810\n",
      "Average accuracy of 10 flod cross validation: 0.985515\n",
      "--------------------\n",
      "Dropout keep rate: 1.000000\n",
      "Training with 10-flod cross validation\n",
      "--------------------\n",
      "Flod number: 1\n",
      "Stop at epoch 5, Final accuracy: 0.980392\n",
      "Label 0 Precision: 0.994819, Recall: 0.987993\n",
      "Label 1 Precision: 0.998532, Recall: 0.981241\n",
      "Label 2 Precision: 0.961083, Recall: 0.967632\n",
      "Label 3 Precision: 0.954058, Recall: 0.985759\n",
      "Label 4 Precision: 0.983813, Recall: 0.968142\n",
      "Flod number: 2\n",
      "Stop at epoch 9, Final accuracy: 0.985948\n",
      "Label 0 Precision: 0.977492, Recall: 0.998358\n",
      "Label 1 Precision: 0.992212, Recall: 0.987597\n",
      "Label 2 Precision: 0.972625, Recall: 0.975767\n",
      "Label 3 Precision: 0.989744, Recall: 0.969849\n",
      "Label 4 Precision: 0.991525, Recall: 0.991525\n",
      "Flod number: 3\n",
      "Stop at epoch 7, Final accuracy: 0.983333\n",
      "Label 0 Precision: 0.996516, Recall: 0.954925\n",
      "Label 1 Precision: 0.985359, Recall: 0.985359\n",
      "Label 2 Precision: 0.957118, Recall: 0.968750\n",
      "Label 3 Precision: 0.977671, Recall: 0.993517\n",
      "Label 4 Precision: 0.983137, Recall: 0.996581\n",
      "Flod number: 4\n",
      "Stop at epoch 14, Final accuracy: 0.988889\n",
      "Label 0 Precision: 0.994792, Recall: 0.991349\n",
      "Label 1 Precision: 0.991004, Recall: 0.988042\n",
      "Label 2 Precision: 0.955446, Recall: 0.988055\n",
      "Label 3 Precision: 0.987159, Recall: 0.970032\n",
      "Label 4 Precision: 0.989796, Recall: 0.981450\n",
      "Flod number: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop at epoch 11, Final accuracy: 0.988235\n",
      "Label 0 Precision: 0.983793, Recall: 0.990212\n",
      "Label 1 Precision: 0.995763, Recall: 0.984637\n",
      "Label 2 Precision: 0.964103, Recall: 0.992958\n",
      "Label 3 Precision: 0.989899, Recall: 0.986577\n",
      "Label 4 Precision: 0.989209, Recall: 0.970018\n",
      "Flod number: 6\n",
      "Stop at epoch 6, Final accuracy: 0.982680\n",
      "Label 0 Precision: 0.991667, Recall: 0.972222\n",
      "Label 1 Precision: 0.981402, Recall: 0.995646\n",
      "Label 2 Precision: 0.978512, Recall: 0.972085\n",
      "Label 3 Precision: 0.993266, Recall: 0.976821\n",
      "Label 4 Precision: 0.964413, Recall: 0.992674\n",
      "Flod number: 7\n",
      "Stop at epoch 13, Final accuracy: 0.990520\n",
      "Label 0 Precision: 0.996587, Recall: 0.986486\n",
      "Label 1 Precision: 0.981453, Recall: 0.998428\n",
      "Label 2 Precision: 0.983498, Recall: 0.986755\n",
      "Label 3 Precision: 0.996732, Recall: 0.980707\n",
      "Label 4 Precision: 0.986842, Recall: 0.991736\n",
      "Flod number: 8\n",
      "Stop at epoch 14, Final accuracy: 0.991827\n",
      "Label 0 Precision: 0.996479, Recall: 0.984348\n",
      "Label 1 Precision: 0.998221, Recall: 0.855183\n",
      "Label 2 Precision: 0.985737, Recall: 0.977987\n",
      "Label 3 Precision: 0.848101, Recall: 0.996694\n",
      "Label 4 Precision: 0.994889, Recall: 0.994889\n",
      "Flod number: 9\n",
      "Stop at epoch 11, Final accuracy: 0.984309\n",
      "Label 0 Precision: 0.986577, Recall: 0.994924\n",
      "Label 1 Precision: 0.991342, Recall: 0.989914\n",
      "Label 2 Precision: 0.972835, Recall: 0.971186\n",
      "Label 3 Precision: 0.974277, Recall: 0.990196\n",
      "Label 4 Precision: 0.994633, Recall: 0.972028\n",
      "Flod number: 10\n",
      "Stop at epoch 12, Final accuracy: 0.988885\n",
      "Label 0 Precision: 0.994755, Recall: 0.996497\n",
      "Label 1 Precision: 0.989458, Recall: 0.993949\n",
      "Label 2 Precision: 0.967960, Recall: 0.984563\n",
      "Label 3 Precision: 0.991722, Recall: 0.978758\n",
      "Label 4 Precision: 0.993610, Recall: 0.984177\n",
      "Average accuracy of 10 flod cross validation: 0.985679\n",
      "-------------------------------------------------------------------\n",
      "Best dropout keep rate after 10-flod cross validation: 1.000000\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "#preprocess the data\n",
    "\n",
    "#combine train and validation set\n",
    "X_Cross = tf.concat([X_train1,X_valid1],0) #shape = (30596,784)\n",
    "y_Cross = tf.concat([y_train1,y_valid1],0)\n",
    "kf = KFold(n_splits=10,shuffle=True)\n",
    "kf.get_n_splits(X_Cross)\n",
    "flod = 0\n",
    "\n",
    "Cross_valid_flod_accuracy = []\n",
    "dropout_accuracy = []\n",
    "Test_accuracy = []\n",
    "\n",
    "for dropout_rate in range(5, 11, 1):\n",
    "    print('--------------------')\n",
    "    print('Dropout keep rate: %f' %(dropout_rate/10))\n",
    "    print('Training with 10-flod cross validation')\n",
    "    print('--------------------')\n",
    "    flod = 0\n",
    "    for train_index, valid_index in kf.split(sess.run(X_Cross)):\n",
    "        print('Flod number: %d'%(flod+1))\n",
    "        flod += 1\n",
    "        X_train, X_valid = sess.run(X_Cross)[train_index], sess.run(X_Cross)[valid_index]\n",
    "        y_train, y_valid = sess.run(y_Cross)[train_index], sess.run(y_Cross)[valid_index]\n",
    "\n",
    "        y_train =  one_hot(y_train)\n",
    "        y_valid =  one_hot(y_valid)\n",
    "\n",
    "        #define model\n",
    "        INPUT_NODE = 784\n",
    "        NEURONS = 128\n",
    "        OUTPUT_NODE = 5\n",
    "\n",
    "        #define loss (cost) function\n",
    "        true_labels = tf.placeholder(tf.int32, [None,5])\n",
    "        x = tf.placeholder(tf.float32, [None,INPUT_NODE])\n",
    "        dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        if(flod-1 == 0):\n",
    "            reuse = False\n",
    "        else:\n",
    "            reuse = True\n",
    "\n",
    "        output, output_after_softmax = training(x,INPUT_NODE,NEURONS,reuse,dropout_keep_prob)\n",
    "\n",
    "        #better take unsoftmax output as logits as our cross entropy loss function will do softmax\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits= output,labels= tf.argmax(true_labels, 1))) \n",
    "\n",
    "        with tf.variable_scope('Adam',reuse=tf.AUTO_REUSE) as scope:\n",
    "            train_step = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(output_after_softmax, 1), tf.argmax(true_labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        #Training and Testing\n",
    "\n",
    "        # define training parameters\n",
    "        training_epochs = 15\n",
    "        batch_size = 500\n",
    "        train_data_size = np.prod(X_train.shape[0]) #28038\n",
    "        total_batch = int(train_data_size/batch_size)\n",
    "\n",
    "        # prepare session\n",
    "        sess = tf.InteractiveSession()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Training cycle\n",
    "        current_best = 0.0\n",
    "        stopping_step = 0\n",
    "        TP_0 = 0\n",
    "        FP_0 = 0\n",
    "        FN_0 = 0\n",
    "        TP_1 = 0\n",
    "        FP_1 = 0\n",
    "        FN_1 = 0\n",
    "        TP_2 = 0\n",
    "        FP_2 = 0\n",
    "        FN_2 = 0\n",
    "        TP_3 = 0\n",
    "        FP_3 = 0\n",
    "        FN_3 = 0\n",
    "        TP_4 = 0\n",
    "        FP_4 = 0\n",
    "        FN_4 = 0\n",
    "        precision_0 = 0.0\n",
    "        recall_0 = 0.0\n",
    "        precision_1 = 0.0\n",
    "        recall_1 = 0.0\n",
    "        precision_2 = 0.0\n",
    "        recall_2 = 0.0\n",
    "        precision_3 = 0.0\n",
    "        recall_3 = 0.0\n",
    "        precision_4 = 0.0\n",
    "        recall_4 = 0.0\n",
    "        current_best = 0.0\n",
    "        epoch_number = 0;\n",
    "        for epoch in range(training_epochs):\n",
    "            #print ('epoch %f in %f'% (epoch+1, training_epochs))\n",
    "            epoch_number+=1\n",
    "            # generate random indexes\n",
    "            indexes = np.random.permutation(train_data_size)\n",
    "\n",
    "            # run all datas for one epoch\n",
    "            for position in range(0, train_data_size, batch_size):\n",
    "                # generate mini batch ids\n",
    "                ids = indexes[position:(position+batch_size) if (position+batch_size) < train_data_size else train_data_size]\n",
    "                batch_xs = X_train[ids]\n",
    "                batch_ts = y_train[ids]\n",
    "\n",
    "                sess.run(train_step,feed_dict={x:batch_xs, true_labels:batch_ts, dropout_keep_prob: dropout_rate/10})\n",
    "\n",
    "            # now we test model per epoch\n",
    "            loss_val, acc_val = sess.run([loss, accuracy],feed_dict={x:X_valid, true_labels:y_valid, dropout_keep_prob: 1})\n",
    "            predicted_result = sess.run(output_after_softmax,feed_dict={x:X_valid, true_labels:y_valid, dropout_keep_prob: 1})\n",
    "\n",
    "            fianl_predict = np.argmax(predicted_result,1)\n",
    "            labels = np.argmax(y_valid,1)\n",
    "            \n",
    "            TP_0 = 0\n",
    "            FP_0 = 0\n",
    "            FN_0 = 0\n",
    "            TP_1 = 0\n",
    "            FP_1 = 0\n",
    "            FN_1 = 0\n",
    "            TP_2 = 0\n",
    "            FP_2 = 0\n",
    "            FN_2 = 0\n",
    "            TP_3 = 0\n",
    "            FP_3 = 0\n",
    "            FN_3 = 0\n",
    "            TP_4 = 0\n",
    "            FP_4 = 0\n",
    "            FN_4 = 0\n",
    "            \n",
    "            \n",
    "            for i in range(0,fianl_predict.size,1):\n",
    "                if(fianl_predict[i] == 0):\n",
    "                    if(labels[i] == 0):\n",
    "                        TP_0+=1\n",
    "                    else:\n",
    "                        FP_0+=1\n",
    "                if(fianl_predict[i] == 1):\n",
    "                    if(labels[i] == 1):\n",
    "                        TP_1+=1\n",
    "                    else:\n",
    "                        FP_1+=1\n",
    "                if(fianl_predict[i] == 2):\n",
    "                    if(labels[i] == 2):\n",
    "                        TP_2+=1\n",
    "                    else:\n",
    "                        FP_2+=1\n",
    "                if(fianl_predict[i] == 3):\n",
    "                    if(labels[i] == 3):\n",
    "                        TP_3+=1\n",
    "                    else:\n",
    "                        FP_3+=1\n",
    "                if(fianl_predict[i] == 4):\n",
    "                    if(labels[i] == 4):\n",
    "                        TP_4+=1\n",
    "                    else:\n",
    "                        FP_4+=1\n",
    "                if(labels[i] == 0):\n",
    "                    if(fianl_predict[i] != 0):\n",
    "                        FN_0+=1\n",
    "                if (labels[i] == 1):\n",
    "                    if(fianl_predict[i] != 1):\n",
    "                        FN_1+=1\n",
    "                if (labels[i] == 2):\n",
    "                    if(fianl_predict[i] != 2):\n",
    "                        FN_2+=1\n",
    "                if (labels[i] == 3):\n",
    "                    if(fianl_predict[i] != 3):\n",
    "                        FN_3+=1\n",
    "                if (labels[i] == 4):\n",
    "                    if(fianl_predict[i] != 4):\n",
    "                        FN_4+=1\n",
    "\n",
    "            #print ('epoch: %d, Loss: %f, Accuracy: %f'% (epoch+1, loss_val, acc_val))\n",
    "\n",
    "            if(acc_val > current_best):\n",
    "                current_best = acc_val\n",
    "                stopping_step = 0\n",
    "            else:\n",
    "                stopping_step +=1\n",
    "                \n",
    "            if(stopping_step >= 3):\n",
    "                break\n",
    "                \n",
    "        precision_0 = TP_0/(TP_0 + FP_0)\n",
    "        recall_0 = TP_0/(TP_0 + FN_0)\n",
    "        precision_1 = TP_1/(TP_1 + FP_1)\n",
    "        recall_1 = TP_1/(TP_1 + FN_1)\n",
    "        precision_2 = TP_2/(TP_2 + FP_2)\n",
    "        recall_2 = TP_2/(TP_2 + FN_2)\n",
    "        precision_3 = TP_3/(TP_3 + FP_3)\n",
    "        recall_3 = TP_3/(TP_3 + FN_3)\n",
    "        precision_4 = TP_4/(TP_4 + FP_4)\n",
    "        recall_4 = TP_4/(TP_4 + FN_4)\n",
    "        print ('Stop at epoch %d, Final accuracy: %f'% (epoch_number-1,current_best))\n",
    "        print ('Label 0 Precision: %f, Recall: %f'%(precision_0,recall_0))\n",
    "        print ('Label 1 Precision: %f, Recall: %f'%(precision_1,recall_1))\n",
    "        print ('Label 2 Precision: %f, Recall: %f'%(precision_2,recall_2))\n",
    "        print ('Label 3 Precision: %f, Recall: %f'%(precision_3,recall_3))\n",
    "        print ('Label 4 Precision: %f, Recall: %f'%(precision_4,recall_4))\n",
    "        Cross_valid_flod_accuracy.append(current_best)\n",
    "        \n",
    "                \n",
    "    Cross_valid_avg_accuracy = np.array(Cross_valid_flod_accuracy)\n",
    "    print('Average accuracy of 10 flod cross validation: %f' %np.mean(Cross_valid_avg_accuracy))\n",
    "    dropout_accuracy.append(np.mean(Cross_valid_avg_accuracy))\n",
    "\n",
    "dropout_accuracy_best = np.array(dropout_accuracy)\n",
    "print('-------------------------------------------------------------------')\n",
    "print('Best dropout keep rate after 10-flod cross validation: %f' %((np.argmax(dropout_accuracy_best)+5)/10))\n",
    "print('-------------------------------------------------------------------')\n",
    "saver.save(sess, './model/dnn_session/Team46_HW2.ckpt')\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.992800\n",
      "Label 0 Precision: 0.996929, Recall: 0.993878\n",
      "Label 1 Precision: 0.994723, Recall: 0.996476\n",
      "Label 2 Precision: 0.987366, Recall: 0.984496\n",
      "Label 3 Precision: 0.988142, Recall: 0.990099\n",
      "Label 4 Precision: 0.993902, Recall: 0.995927\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "INPUT_NODE = 784\n",
    "NEURONS = 128\n",
    "OUTPUT_NODE = 5\n",
    "\n",
    "#define loss (cost) function\n",
    "true_labels = tf.placeholder(tf.int32, [None,5])\n",
    "x = tf.placeholder(tf.float32, [None,INPUT_NODE])\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "output, output_after_softmax = training(x,INPUT_NODE,NEURONS,False,1.0)\n",
    "\n",
    "#better take unsoftmax output as logits as our cross entropy loss function will do softmax\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits= output,labels= tf.argmax(true_labels, 1))) \n",
    "\n",
    "with tf.variable_scope('Adam',reuse=tf.AUTO_REUSE) as scope:\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(output_after_softmax, 1), tf.argmax(true_labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# define training parameters\n",
    "training_epochs = 15\n",
    "batch_size = 500\n",
    "train_data_size = np.prod(X_train1.shape[0]) #28038\n",
    "total_batch = int(train_data_size/batch_size)\n",
    "\n",
    "sess2 = tf.InteractiveSession()\n",
    "sess2.run(tf.global_variables_initializer())\n",
    "saver2 = tf.train.Saver()\n",
    "\n",
    "# Training cycle\n",
    "current_best = 0.0\n",
    "stopping_step = 0\n",
    "TP_0 = 0\n",
    "FP_0 = 0\n",
    "FN_0 = 0\n",
    "TP_1 = 0\n",
    "FP_1 = 0\n",
    "FN_1 = 0\n",
    "TP_2 = 0\n",
    "FP_2 = 0\n",
    "FN_2 = 0\n",
    "TP_3 = 0\n",
    "FP_3 = 0\n",
    "FN_3 = 0\n",
    "TP_4 = 0\n",
    "FP_4 = 0\n",
    "FN_4 = 0\n",
    "precision_0 = 0.0\n",
    "recall_0 = 0.0\n",
    "precision_1 = 0.0\n",
    "recall_1 = 0.0\n",
    "precision_2 = 0.0\n",
    "recall_2 = 0.0\n",
    "precision_3 = 0.0\n",
    "recall_3 = 0.0\n",
    "precision_4 = 0.0\n",
    "recall_4 = 0.0\n",
    "epoch_number = 0;\n",
    "y_train1 =  one_hot(y_train1)\n",
    "y_test1 =  one_hot(y_test1)\n",
    "for epoch in range(training_epochs):\n",
    "    #print ('epoch %f in %f'% (epoch+1, training_epochs))\n",
    "    epoch_number+=1\n",
    "    # generate random indexes\n",
    "    indexes = np.random.permutation(train_data_size)\n",
    "\n",
    "\n",
    "    # run all datas for one epoch\n",
    "    for position in range(0, train_data_size, batch_size):\n",
    "        # generate mini batch ids\n",
    "        ids = indexes[position:(position+batch_size) if (position+batch_size) < train_data_size else train_data_size]\n",
    "        batch_xs = X_train1[ids]\n",
    "        batch_ts = y_train1[ids]\n",
    "\n",
    "        sess2.run(train_step,feed_dict={x:batch_xs, true_labels:batch_ts, dropout_keep_prob: 1})\n",
    "\n",
    "    loss_val, acc_val = sess2.run([loss, accuracy],feed_dict={x:X_test1, true_labels:y_test1, dropout_keep_prob: 1})\n",
    "    predicted_result = sess2.run(output_after_softmax,feed_dict={x:X_test1, true_labels:y_test1, dropout_keep_prob: 1})\n",
    "\n",
    "    fianl_predict = np.argmax(predicted_result,1)\n",
    "    labels = np.argmax(y_test1,1)\n",
    "    \n",
    "    if(acc_val > current_best):\n",
    "        current_best = acc_val\n",
    "\n",
    "    TP_0 = 0\n",
    "    FP_0 = 0\n",
    "    FN_0 = 0\n",
    "    TP_1 = 0\n",
    "    FP_1 = 0\n",
    "    FN_1 = 0\n",
    "    TP_2 = 0\n",
    "    FP_2 = 0\n",
    "    FN_2 = 0\n",
    "    TP_3 = 0\n",
    "    FP_3 = 0\n",
    "    FN_3 = 0\n",
    "    TP_4 = 0\n",
    "    FP_4 = 0\n",
    "    FN_4 = 0\n",
    "\n",
    "\n",
    "    for i in range(0,fianl_predict.size,1):\n",
    "        if(fianl_predict[i] == 0):\n",
    "            if(labels[i] == 0):\n",
    "                TP_0+=1\n",
    "            else:\n",
    "                FP_0+=1\n",
    "        if(fianl_predict[i] == 1):\n",
    "            if(labels[i] == 1):\n",
    "                TP_1+=1\n",
    "            else:\n",
    "                FP_1+=1\n",
    "        if(fianl_predict[i] == 2):\n",
    "            if(labels[i] == 2):\n",
    "                TP_2+=1\n",
    "            else:\n",
    "                FP_2+=1\n",
    "        if(fianl_predict[i] == 3):\n",
    "            if(labels[i] == 3):\n",
    "                TP_3+=1\n",
    "            else:\n",
    "                FP_3+=1\n",
    "        if(fianl_predict[i] == 4):\n",
    "            if(labels[i] == 4):\n",
    "                TP_4+=1\n",
    "            else:\n",
    "                FP_4+=1\n",
    "        if(labels[i] == 0):\n",
    "            if(fianl_predict[i] != 0):\n",
    "                FN_0+=1\n",
    "        if (labels[i] == 1):\n",
    "            if(fianl_predict[i] != 1):\n",
    "                FN_1+=1\n",
    "        if (labels[i] == 2):\n",
    "            if(fianl_predict[i] != 2):\n",
    "                FN_2+=1\n",
    "        if (labels[i] == 3):\n",
    "            if(fianl_predict[i] != 3):\n",
    "                FN_3+=1\n",
    "        if (labels[i] == 4):\n",
    "            if(fianl_predict[i] != 4):\n",
    "                FN_4+=1\n",
    "\n",
    "precision_0 = TP_0/(TP_0 + FP_0)\n",
    "recall_0 = TP_0/(TP_0 + FN_0)\n",
    "precision_1 = TP_1/(TP_1 + FP_1)\n",
    "recall_1 = TP_1/(TP_1 + FN_1)\n",
    "precision_2 = TP_2/(TP_2 + FP_2)\n",
    "recall_2 = TP_2/(TP_2 + FN_2)\n",
    "precision_3 = TP_3/(TP_3 + FP_3)\n",
    "recall_3 = TP_3/(TP_3 + FN_3)\n",
    "precision_4 = TP_4/(TP_4 + FP_4)\n",
    "recall_4 = TP_4/(TP_4 + FN_4)\n",
    "print ('Test accuracy: %f'% (current_best))\n",
    "print ('Label 0 Precision: %f, Recall: %f'%(precision_0,recall_0))\n",
    "print ('Label 1 Precision: %f, Recall: %f'%(precision_1,recall_1))\n",
    "print ('Label 2 Precision: %f, Recall: %f'%(precision_2,recall_2))\n",
    "print ('Label 3 Precision: %f, Recall: %f'%(precision_3,recall_3))\n",
    "print ('Label 4 Precision: %f, Recall: %f'%(precision_4,recall_4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
