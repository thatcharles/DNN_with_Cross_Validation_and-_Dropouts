{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as sk\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "###### Do not modify here ###### \n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# training on MNIST but only on digits 0 to 4\n",
    "X_train1 = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5]\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5]\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5]\n",
    "\n",
    "###### Do not modify here ###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform to one_hot array\n",
    "def one_hot(input_data):\n",
    "    one_hot = []\n",
    "    for item in input_data:\n",
    "        if item == 0.:\n",
    "            one_h = [1.,0.,0.,0.,0.]\n",
    "        elif item == 1.:\n",
    "            one_h = [0.,1.,0.,0.,0.]\n",
    "        elif item == 2.:\n",
    "            one_h = [0.,0.,1.,0.,0.]\n",
    "        elif item == 3.:\n",
    "            one_h = [0.,0.,0.,1.,0.]\n",
    "        elif item == 4.:\n",
    "            one_h = [0.,0.,0.,0.,1.]\n",
    "\n",
    "        one_hot.append(one_h)\n",
    "    one_hot = np.array(one_hot)\n",
    "    #one_hot = one_hot.reshape(len(one_hot),10,1)\n",
    "    #one_hot = one_hot.reshape(len(one_hot), 7,1)\n",
    "    #return tf.constant([one_hot])\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and cross validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.991633\n",
      "Label 0 Precision: 0.993871, Recall: 0.992857\n",
      "Label 1 Precision: 0.999116, Recall: 0.995595\n",
      "Label 2 Precision: 0.988911, Recall: 0.950581\n",
      "Label 3 Precision: 0.951796, Recall: 0.997030\n",
      "Label 4 Precision: 0.992850, Recall: 0.989817\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "INPUT_NODE = 784\n",
    "NEURONS = 128\n",
    "OUTPUT_NODE = 5\n",
    "\n",
    "#define loss (cost) function\n",
    "true_labels = tf.placeholder(tf.int32, [None,5], name='TL')\n",
    "x = tf.placeholder(tf.float32, [None,INPUT_NODE], name='X')\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name='dropout')\n",
    "\n",
    "initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG', uniform=True)\n",
    "\n",
    "#W_fc1 = tf.Variable(initializer([INPUT_NODE, NEURONS]), name='WLayer1')  # Weights\n",
    "#b_fc1 = tf.Variable(initializer([NEURONS]), name='bLayer1')  # bias\n",
    "#h_fc1 = tf.nn.elu(tf.matmul(x, W_fc1) + b_fc1, name='Layer1') # y = Wx + b\n",
    "\n",
    "h_fc1 = tf.layers.dense(inputs=x, units=NEURONS, kernel_initializer=initializer,activation=tf.nn.relu, name='Layer1')\n",
    "\n",
    "#W_fc2 = tf.Variable(initializer([NEURONS, NEURONS]), name='WLayer2')  # Weights\n",
    "#b_fc2 = tf.Variable(initializer([NEURONS]), name='bLayer2')  # bias\n",
    "#h_fc2 = tf.nn.elu(tf.matmul(h_fc1, W_fc2) + b_fc2, name='Layer2') # y = Wx + b\n",
    "\n",
    "h_fc2 = tf.layers.dense(inputs=h_fc1, units=NEURONS, kernel_initializer=initializer,activation=tf.nn.relu, name='Layer2')\n",
    "\n",
    "W_fc3 = tf.Variable(initializer([NEURONS, NEURONS]), name='WLayer3')  # Weights\n",
    "b_fc3 = tf.Variable(initializer([NEURONS]), name='bLayer3')  # bias\n",
    "h_fc3 = tf.nn.elu(tf.matmul(h_fc2, W_fc3) + b_fc3, name='Layer3') # y = Wx + b\n",
    "\n",
    "W_fc4 = tf.Variable(initializer([NEURONS, NEURONS]), name='WLayer4')  # Weights\n",
    "b_fc4 = tf.Variable(initializer([NEURONS]), name='bLayer4')  # bias\n",
    "h_fc4 = tf.nn.elu(tf.matmul(h_fc3, W_fc4) + b_fc4, name='Layer4') # y = Wx + b\n",
    "\n",
    "W_fc5 = tf.Variable(initializer([NEURONS, NEURONS]), name='WLayer5')  # Weights\n",
    "b_fc5 = tf.Variable(initializer([NEURONS]), name='bLayer5')  # bias\n",
    "h_fc5 = tf.nn.elu(tf.matmul(h_fc4, W_fc5) + b_fc5, name='Layer5') # y = Wx + b\n",
    "\n",
    "output = tf.layers.dense(inputs=h_fc5, units=OUTPUT_NODE, kernel_initializer=initializer, name='output')\n",
    "output_after_softmax = tf.nn.softmax(output, name='output_after_softmax')\n",
    "\n",
    "#better take unsoftmax output as logits as our cross entropy loss function will do softmax\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits= output,labels= tf.argmax(true_labels, 1)), name='loss') \n",
    "\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "tf.add_to_collection(\"optimizer\", train_step)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(output_after_softmax, 1), tf.argmax(true_labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "\n",
    "# define training parameters\n",
    "training_epochs = 15\n",
    "batch_size = 500\n",
    "train_data_size = np.prod(X_train1.shape[0]) #28038\n",
    "total_batch = int(train_data_size/batch_size)\n",
    "\n",
    "sess2 = tf.InteractiveSession()\n",
    "sess2.run(tf.global_variables_initializer())\n",
    "saver2 = tf.train.Saver()\n",
    "\n",
    "# Training cycle\n",
    "current_best = 0.0\n",
    "stopping_step = 0\n",
    "TP_0 = 0\n",
    "FP_0 = 0\n",
    "FN_0 = 0\n",
    "TP_1 = 0\n",
    "FP_1 = 0\n",
    "FN_1 = 0\n",
    "TP_2 = 0\n",
    "FP_2 = 0\n",
    "FN_2 = 0\n",
    "TP_3 = 0\n",
    "FP_3 = 0\n",
    "FN_3 = 0\n",
    "TP_4 = 0\n",
    "FP_4 = 0\n",
    "FN_4 = 0\n",
    "precision_0 = 0.0\n",
    "recall_0 = 0.0\n",
    "precision_1 = 0.0\n",
    "recall_1 = 0.0\n",
    "precision_2 = 0.0\n",
    "recall_2 = 0.0\n",
    "precision_3 = 0.0\n",
    "recall_3 = 0.0\n",
    "precision_4 = 0.0\n",
    "recall_4 = 0.0\n",
    "epoch_number = 0;\n",
    "y_train1 =  one_hot(y_train1)\n",
    "y_test1 =  one_hot(y_test1)\n",
    "for epoch in range(training_epochs):\n",
    "    #print ('epoch %f in %f'% (epoch+1, training_epochs))\n",
    "    epoch_number+=1\n",
    "    # generate random indexes\n",
    "    indexes = np.random.permutation(train_data_size)\n",
    "\n",
    "\n",
    "    # run all datas for one epoch\n",
    "    for position in range(0, train_data_size, batch_size):\n",
    "        # generate mini batch ids\n",
    "        ids = indexes[position:(position+batch_size) if (position+batch_size) < train_data_size else train_data_size]\n",
    "        batch_xs = X_train1[ids]\n",
    "        batch_ts = y_train1[ids]\n",
    "\n",
    "        sess2.run(train_step,feed_dict={x:batch_xs, true_labels:batch_ts, dropout_keep_prob: 1})\n",
    "\n",
    "    loss_val, acc_val = sess2.run([loss, accuracy],feed_dict={x:X_test1, true_labels:y_test1, dropout_keep_prob: 1})\n",
    "    predicted_result = sess2.run(output_after_softmax,feed_dict={x:X_test1, true_labels:y_test1, dropout_keep_prob: 1})\n",
    "\n",
    "    fianl_predict = np.argmax(predicted_result,1)\n",
    "    labels = np.argmax(y_test1,1)\n",
    "    \n",
    "    if(acc_val > current_best):\n",
    "        current_best = acc_val\n",
    "\n",
    "    TP_0 = 0\n",
    "    FP_0 = 0\n",
    "    FN_0 = 0\n",
    "    TP_1 = 0\n",
    "    FP_1 = 0\n",
    "    FN_1 = 0\n",
    "    TP_2 = 0\n",
    "    FP_2 = 0\n",
    "    FN_2 = 0\n",
    "    TP_3 = 0\n",
    "    FP_3 = 0\n",
    "    FN_3 = 0\n",
    "    TP_4 = 0\n",
    "    FP_4 = 0\n",
    "    FN_4 = 0\n",
    "\n",
    "\n",
    "    for i in range(0,fianl_predict.size,1):\n",
    "        if(fianl_predict[i] == 0):\n",
    "            if(labels[i] == 0):\n",
    "                TP_0+=1\n",
    "            else:\n",
    "                FP_0+=1\n",
    "        if(fianl_predict[i] == 1):\n",
    "            if(labels[i] == 1):\n",
    "                TP_1+=1\n",
    "            else:\n",
    "                FP_1+=1\n",
    "        if(fianl_predict[i] == 2):\n",
    "            if(labels[i] == 2):\n",
    "                TP_2+=1\n",
    "            else:\n",
    "                FP_2+=1\n",
    "        if(fianl_predict[i] == 3):\n",
    "            if(labels[i] == 3):\n",
    "                TP_3+=1\n",
    "            else:\n",
    "                FP_3+=1\n",
    "        if(fianl_predict[i] == 4):\n",
    "            if(labels[i] == 4):\n",
    "                TP_4+=1\n",
    "            else:\n",
    "                FP_4+=1\n",
    "        if(labels[i] == 0):\n",
    "            if(fianl_predict[i] != 0):\n",
    "                FN_0+=1\n",
    "        if (labels[i] == 1):\n",
    "            if(fianl_predict[i] != 1):\n",
    "                FN_1+=1\n",
    "        if (labels[i] == 2):\n",
    "            if(fianl_predict[i] != 2):\n",
    "                FN_2+=1\n",
    "        if (labels[i] == 3):\n",
    "            if(fianl_predict[i] != 3):\n",
    "                FN_3+=1\n",
    "        if (labels[i] == 4):\n",
    "            if(fianl_predict[i] != 4):\n",
    "                FN_4+=1\n",
    "\n",
    "precision_0 = TP_0/(TP_0 + FP_0)\n",
    "recall_0 = TP_0/(TP_0 + FN_0)\n",
    "precision_1 = TP_1/(TP_1 + FP_1)\n",
    "recall_1 = TP_1/(TP_1 + FN_1)\n",
    "precision_2 = TP_2/(TP_2 + FP_2)\n",
    "recall_2 = TP_2/(TP_2 + FN_2)\n",
    "precision_3 = TP_3/(TP_3 + FP_3)\n",
    "recall_3 = TP_3/(TP_3 + FN_3)\n",
    "precision_4 = TP_4/(TP_4 + FP_4)\n",
    "recall_4 = TP_4/(TP_4 + FN_4)\n",
    "print ('Test accuracy: %f'% (current_best))\n",
    "print ('Label 0 Precision: %f, Recall: %f'%(precision_0,recall_0))\n",
    "print ('Label 1 Precision: %f, Recall: %f'%(precision_1,recall_1))\n",
    "print ('Label 2 Precision: %f, Recall: %f'%(precision_2,recall_2))\n",
    "print ('Label 3 Precision: %f, Recall: %f'%(precision_3,recall_3))\n",
    "print ('Label 4 Precision: %f, Recall: %f'%(precision_4,recall_4))\n",
    "saver2.save(sess2, './checkpointFile/my_test_model')\n",
    "sess2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
